# Clustering and classification

*We learnt how to perform scaling of data so that they are more comparable. We also did linear discriminate analysis and k-means clustering .*


#########################################################################################
##BOSTON Data Variables
#########################################################################################

```{r read data, eval=TRUE, message = FALSE, warnings= FALSE, echo=FALSE}
# load the data
library(ggplot2)
library(MASS)
library(tidyverse)

data("Boston")

# explore the dataset
str(Boston)
print("Summary before scaling")
summary(Boston)
dim(Boston)
```


The Boston dataset has 506 rows and 14 variables. This dataset has information about the city of Boston. Information includes crime rate (crim),information about residents: proportion of residential land (zn), average number of rooms per dwelling (rm), proportion of owner-occupied homes (age), value of owner-occupied homes (medv), percentage of black people (black) and lowers status of population (lstat). There is also information about: business areas (indus), distance to employment centers (dis), pupil-teacher ration (ptratio), property tax(tax) and accessibility to highway (rad). All data are of type num except for chas and rad which are integers.

##Relationship between variables

```{r plot data, eval=TRUE, message = FALSE, warnings= FALSE, echo=FALSE}
# access the GGally and ggplot2 libraries
library(GGally)
library(ggplot2)

# create a more advanced plot matrix with ggpairs(learning2014[-1])
p <- ggpairs(Boston, mapping = aes( alpha=0.3), lower = list(combo = wrap("facethist", bins = 20)))

# draw the plot
p

```
The Boston dataset plot shows that most of the distributions of the variables are not normal.


```{r correlation of variables, eval=TRUE, message = FALSE, warnings= FALSE, echo=FALSE}

# calculate the correlation matrix and round it
cor_matrix<-cor(Boston) 

# print the correlation matrix
cor_matrix %>% round(2)  %>% print() 

library(corrplot)

cor.mtest <- function(mat, conf.level = 0.95){
  mat <- as.matrix(mat)
  n <- ncol(mat)
  p.mat <- lowCI.mat <- uppCI.mat <- matrix(NA, n, n)
  diag(p.mat) <- 0
  diag(lowCI.mat) <- diag(uppCI.mat) <- 1
  for(i in 1:(n-1)){
    for(j in (i+1):n){
      tmp <- cor.test(mat[,i], mat[,j], conf.level = conf.level)
      p.mat[i,j] <- p.mat[j,i] <- tmp$p.value
      lowCI.mat[i,j] <- lowCI.mat[j,i] <- tmp$conf.int[1]
      uppCI.mat[i,j] <- uppCI.mat[j,i] <- tmp$conf.int[2]
    }
  }
  return(list(p.mat, lowCI.mat, uppCI.mat))
}

res1 <- cor.mtest(Boston,0.95)
# visualize the correlation matrix
corrplot.mixed(cor_matrix,tl.cex=0.5,p.mat = res1[[1]], insig = "blank"
,mar=c(1,0,2,0),number.cex=0.7, title="Correlation between variables")

```

The table above shows the correlations between the variables. The correlation plot below it shows the value of the above correlations with circles: the bigger the circle, the higher the correlation. The p-values of the correlation are on the lower diagonal.

The highest positive correlation is between tax and rad (0.91) followed by age and nox (0.73). The highest negative correlation
is between dis and nox (-0.77) dis and age (-0.75). This makes sense since one would assume that properties that are
more accessible to highways would be of higher value and hence have higher property tax. It is unclear why owner-occupied
homes have higher nitgogen oxside concentrations but they are highly correlated. The is also negative correlation
between distance to employment centers and proportion of homes built before 1940: maybe this is because most of these
homes are further away from the center and are occupied by older people. There is also a high negative correlation between
distance to employment centers and nitrogen oxide concentration.


##Data Standardization
Summary after scaling of data
```{r scale data, eval=TRUE, message = FALSE, warnings= FALSE, echo=FALSE}
# scale the data
# center and standardize variables
boston_scaled <- scale(Boston)

# summaries of the scaled variables
summary(boston_scaled)

# class of the boston_scaled object
class(boston_scaled)

# change the object to data frame
boston_scaled=as.data.frame(boston_scaled)

# create a more advanced plot matrix with ggpairs(learning2014[-1])
p <- ggpairs(boston_scaled, mapping = aes( alpha=0.3), lower = list(combo = wrap("facethist", bins = 20)))

# draw the plot
p

```

```{r chop data , eval=TRUE, message = FALSE, warnings= FALSE, echo=FALSE}
# save the scaled crim as scaled_crim
scaled_crim <- boston_scaled$crim
# summary of the scaled_crim
summary(scaled_crim)
# create a quantile vector of crim and print it
bins <- quantile(scaled_crim)
bins
# create a categorical variable 'crime'
crime <- cut(scaled_crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))
# look at the table of the new factor crime
table(crime)
# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)
# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)
head(boston_scaled)
```
##LDA Analysis
We now need to split out data into test and training. And we do lda on the train data.


```{r test-train data , eval=TRUE, message = FALSE, warnings= FALSE, echo=FALSE}
# number of rows in the Boston dataset 
n <- nrow(boston_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]

# linear discriminant analysis
lda.fit <- lda(crime~., data = train)
# print the lda.fit object
lda.fit
# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}
# target classes as numeric
classes <- as.numeric(train$crime)
# plot the lda results
plot(lda.fit, dimen = 2, col=classes, pch=classes)
lda.arrows(lda.fit, myscale = 3)
```


We now use the model to predict the crime classes in the test data.
```{r test data , eval=TRUE, message = FALSE, warnings= FALSE, echo=FALSE}
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = test$crime, predicted = lda.pred$class)

```

The prediction was highly accurate for the high and low crime rate category but not 
so accurate in the medium classes.

##k-means Clustering.

```{r standarize_scale data , eval=TRUE, message = FALSE, warnings= FALSE, echo=FALSE}
#scaling to same scale using package vegan
library(vegan)
scaled.dat <- decostand(Boston, method = "standardize", MARGIN = 2)

library(MASS)
data('Boston')

# euclidean distance matrix
dist_eu <- dist(scaled.dat)

# determine the number of clusters
k_max <- 10
# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(dist_eu, k)$tot.withinss})
# visualize the results
plot(1:k_max, twcss, type='b')

```

The plot shows that 3 clusters is ideal.

```{r plot k-clusters , eval=TRUE, message = FALSE, warnings= FALSE, echo=FALSE}
# k-means clustering
km <-kmeans(dist_eu, centers = 3)
# plot the Boston dataset with clusters
pairs(scaled.dat, col = km$cluster)

library(cluster) 
clusplot(scaled.dat,km$cluster, color=TRUE, shade=TRUE, labels=2, lines=0)

```

##Checking prediction for clusters (not for crime class!)

```{r check_accuracy , eval=TRUE, message = FALSE, warnings= FALSE, echo=FALSE}
# choose randomly 80% of the rows

library(dplyr); 
library(plyr)

# define 2 new columns in gii data
# number of rows in the Boston dataset 
n <- nrow(scaled.dat)
scaled.dat <- mutate(scaled.dat, cluster = km$cluster)
ind <- sample(n,  size = n * 0.8)

# create train set
train <- scaled.dat[ind,]

# create test set 
test <- scaled.dat[-ind,]

# linear discriminant analysis
lda.fit <- lda(cluster~., data = train)
# print the lda.fit object
lda.fit
# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}
# target classes as numeric
classes <- as.numeric(train$cluster)
# plot the lda results
plot(lda.fit, dimen = 2, col=classes, pch=classes)
lda.arrows(lda.fit, myscale = 3)


```

The clusters are clearly defined in the LDA plot. indus and zn are the most influential
linear seperators.

